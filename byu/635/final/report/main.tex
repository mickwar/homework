\documentclass[12pt]{article}

%\usepackage[longnamesfirst]{natbib}
\usepackage{natbib}
\usepackage[font=footnotesize]{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{listings}

\newcommand{\mc}[1]{\multicolumn{1}{c}{#1}}

\newcommand{\m}[1]{\mathbf{\bm{#1}}}
\newcommand{\R}{I\hspace{-4.4pt}R}

\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}

\begin{document}

\begin{Large}
\begin{center}
GLMM Parameter Estimation
\bigskip

Mickey Warner
\end{center}
\end{Large}

\section*{Introduction}

\noindent In this paper we review ``Approximate Inference in Generalized Linear Mixed Models'' by \cite{breslow:1993}, referred to hereafter as the authors. The generalized linear model (GLM) \citep{mccullagh:1989} allows us to work with non-normal likelihoods, such as for binary, count, or nonnonegative data, by specifying a link function between the mean of a random variable and a systematic component.

For example, suppose $Y$ has some distribution $F$ with mean $\mu$. We could specify a link function $g(\cdot)$ such that $g(\mu)=\m{x}^\top\m{\alpha}$. This form is now familiar to linear regression. The regression coefficients $\m{\alpha}$ are often estimated through some iterative procedure (such as IWLS). These coefficients are then interpreted with respect to the function of the mean. Yet, suitable choices of $g(\cdot)$, with its inverse $h(\cdot)=g^{-1}(\cdot)$, may still produce easily interpretable results.

Linear mixed models \citep{henderson:1959} assume there are additional sources of variation, beyond measurement error. Such models have the form
\[ \m{y} = \m{X}\m{\alpha} + \m{Z}\m{b} + \m{e} \]
where typical assumptions are $\m{b}\sim N(\m{0}, \m{G})$ and $\m{e}\sim N(\m{0}, \m{R})$ with $\m{b}$ and $\m{e}$ independent. These models are useful for repeated measures or blocked data or other complicated designs. The power of these models comes from specifying the covariances $\m{G}$ and $\m{R}$, either as unstructured or having some known form. Many choices of covariance structures are available for use. Methods exist for estimating the fixed and random effects and also the variance-covariance parameters.

Generalized linear mixed models (GLMMs) are a hybrid of the GLM and the linear mixed model. We have a need to use a non-normal likelihood, but also wish to model random effects and covariances. However, analytic solutions often require numerical integration, which becomes infeasible in high dimensions. The authors address the capability of Bayesian methods in avoiding the need for numerical integration. However, they criticize the Bayesian approach due to its need for intense computational resources. Though significant advances in scientific computing and computational resources have made Bayesian methods far more accessible. At the time of their writing (some twenty years ago), their method of parameter estimation and inference was groundbreaking and is still a powerful tool.

The remainder of the paper will proceed as follows. In section 1 we describe in more detail the model that is considered by the authors. Additional theoretical details of the authors' model fitting procedure are provided in section 2 for clarity. We do not go over all of the theory, but give the details the authors glossed over, since some of these have not been covered in our classes. An example is presented in section 3. Section 4 provides concluding remarks.

\section{Hierarchical model}
 
\noindent For an observation $i$, a univariate response $y_i$ is measured with covariates $\m{x}_i$ and $\m{z}_i$ associated with the fixed and random effects, respectively. Units may be blocked in some way. Given a $q$-vector $\m{b}$ of random effects, the responses are assumed to be conditionally independent with means $\E(y_i|\m{b})=\mu_i^b$ and variances $\Var(y_i|\m{b}) = \phi a_i v(\mu_i^b)$, where $\phi$ is a dispersion parameter, $a_i$ is a known constant, and $v(\cdot)$ is a specified variance function. (Note: in $\mu_i^b$, the $b$ is not a power, but a superscript to denote that the mean depends on the random effects.) The conditional mean is related to the linear component $\eta_i^b=\m{x}_i^\top\m{\alpha}+\m{z}_i^\top\m{b}$ through the link function $g(\mu_i^b)=\eta_i^b$, with inverse $h(\cdot)=g^{-1}(\cdot)$, where $\m{\alpha}$ is a $p$-vector of fixed effects.

Denote $\m{y}=(y_1,\ldots,y_n)^\top$ and let $\m{X}$ and $\m{Z}$ have rows comprised of $\m{x}_i^\top$ and $\m{z}_i^\top$. We then write the conditional means using vector notation as
\begin{eqnarray}
\E(\m{y}|\m{b}) = \m{\mu}^b = h(\m{X}\m{\alpha} + \m{Z}\m{b}).
\label{mod}
\end{eqnarray}
Finally, we assume $\m{b}$ is multivariate normally distributed with mean $\m{0}$ and covariance $\m{D}=\m{D}(\m{\theta})$ for an unknown vector $\m{\theta}$ of covariance parameters.

We now give some remarks on this specification. The conditional variance has three terms, $\phi$, $a_i$, and $v(\mu_i^b)$. The dispersion parameter $\phi$ may be estimated, but is occasionally fixed at unity. The authors used $a_i$ as the ``reciprocal of a binomial denominator.'' That is, for experiments involving binomial outcomes, $a_i$ is the number of replicates on each observation. In the examples the authors provide, $a_i$ is fixed at one. The choice of $v(\cdot)$ should be thought of as choosing the likelihood for the data. For example, for count data, $v(\mu_i^b)=\mu_i^b$ is comparable to the Poisson distribution, while $v(\mu_i^b)=\mu_i^b(1-\mu_i^b)$ is similar to the binomial distribution. Note that we are not necessarily making any distributional assumptions about the likelihood. This requires the use of a quasi-likelihood, as discussed in the next section. The link function $g(\cdot)$ must be monotonic and maps the real numbers onto the support for the mean. Choices include the logit or inverse cdf of the normal distribution for binomial data and the natural logarithm for Poisson data. Ease of interpretation and adequacy of model fit should be taken into account when deciding on a link function.

\section{Penalized quasi-likelihood}

\noindent As mentioned earlier, the assumptions made about the conditional mean and variance do not equate to choosing a likelihood. Rather, the link and variance functions amount to specifying first and second moments for the data. The authors propose a method of parameter estimation through various approximations of the penalized quasi-likelihood (PQL) and iterative updates.

Quasi-likelihoods provide an alternative to full-likelihoods. The authors omit the details on quasi-likelihoods, but we will provide some explanation here. \cite{wedderburn:1974} defines the quasi-likelihood of a single observation as
\begin{eqnarray}
\frac{\partial K(y_i| \mu_i^b)}{\partial \mu_i^b} &=& \frac{y_i-\mu_i^b}{a_i v(\mu_i^b)} \label{quasi} \\
\Longrightarrow K(y_i|\mu_i^b)&=&\int_{y_i}^{\mu_i^b}\frac{y_i-u_i}{a_i v(u_i)}du_i, \label{intquasi}
\end{eqnarray}
using the notation from section 1. The left side of the equal sign in (\ref{quasi}) shares similar properties to the derivative of a log-likelihood (the score). For instance, after straightforward calculations, we have
\begin{eqnarray}
\E_Y\left(\frac{\partial K(y_i| \mu_i^b)}{\partial \mu_i^b}\right) &=& 0 \\
\Var_Y\left(\frac{\partial K(y_i| \mu_i^b)}{\partial \mu_i^b}\right)&=& \frac{1}{a_i v(\mu_i^b)}.
\end{eqnarray}
For simple choices of $v(\mu_i^b)$ the integral in (\ref{intquasi}) is tractable. Certain choices also correspond to the log-likelihoods of particular distributions: $v(\mu_i^b)=1$ results in a Gaussian likelihood, $v(\mu_i^b)=\mu_i^b$ a Poisson, $v(\mu_i^b)=(\mu_i^b)^2$ a Gamma, and $v(\mu_i^b)=\mu_i^b(1-\mu_i^b)$ a Binomial, to name a few.

The quasi-likelihood for the set of $n$ observations is the following sum
\begin{eqnarray}
Q(\m{y}|\m{\alpha}, \m{\theta}, \m{b}) = \sum_{i=1}^nK(y_i|\mu_i^b).
\end{eqnarray}
The left side of the equation indicates that the quasi-likelihood is conditional on the parameters and random effects (recall that $\mu_i^b$ is a function of $\m{b}$). Using Bayes theorem, we have the joint distribution
\begin{eqnarray}
ql(\m{y},\m{b}|\m{\alpha},\m{\theta}) = Q(\m{y}|\m{\alpha},\m{\theta},\m{b})p(\m{b}). \label{joint}
\end{eqnarray}
Since $Q(\cdot)$ resembles a log-likelihood, we exponentiate it and then integrate out $\m{b}$ from the joint in (\ref{joint}) to obtain the authors' integrated quasi-likelihood function, their equation (2),
\begin{eqnarray}
e^{ql(\m{y}|\m{\alpha},\m{\theta})} &=& \int\exp\left[Q(\m{y}|\m{\alpha},\m{\theta},\m{b})\right]p(\m{b})d\m{b} \\
&\propto& |\m{D}|^{-1/2}\int\exp\left[\sum_{i=1}^nK(y_i|\mu_i^b)-\m{b}^\top\m{D}^{-1}\m{b}\right]d\m{b}, \label{iq}
\end{eqnarray}
where $p(\m{b})$ is the multivariate normal density with mean $\m{0}$ and covariance matrix $\m{D}$. This is the function used to obtain estimates of $\m{\alpha}$ and $\m{\theta}$. However, there is a clear difficulty in integrating the function. The authors use Laplace's method for integral approximation to obtain a second order approximation of (\ref{iq}).

Laplace's integral approximation is given by
\begin{eqnarray}
\int e^{Mf(x)}dx \approx \sqrt{\frac{2\pi}{M|f''(x_0)|}} e^{Mf(x_0)}
\end{eqnarray}
for large $M$, where $f(x_0)$ is the global maximum of the function $f$. The approximation is generalized when integrating with respect to a vector (see Wikipedia). Let $\kappa(\m{b})=\sum_{i=1}^nK(y_i|\mu_i^b)-\m{b}^\top\m{D}\m{b}$. Let $\kappa'(\m{b})$ and $\kappa''(\m{b})$ be the $q$-vector and $q\times q$ matrix of first- and second-order partial derivatives with respect to $\m{b}$. After applying Laplace's method and taking the log of (\ref{iq}) and ignoring constants, we have
\begin{eqnarray}
ql(\m{y}|\m{\alpha},\m{\theta}) \approx -\frac{1}{2}\log|\m{D}| -\frac{1}{2}\log|\kappa''(\tilde{\m{b}})| - \kappa(\tilde{\m{b}}),
\end{eqnarray}
where $\tilde{\m{b}}$ is the solution to $\kappa'(\m{b})=\m{0}$. From here, the authors solve for the estimating equations for $\m{\alpha}$ and $\m{b}$. An iterated weighted least squares (IWLS) algorithm may be used to obtain estimates for these parameters.

When estimates for $\m{\alpha}$ and $\m{b}$ are calculated, the variance parameters $\m{\theta}$ are estimated through an approximate profile quasi-likelihood. That is, the quasi-likelihood function being maximized is being held constant at the estimates of $\hat{\m{\alpha}}$ and $\hat{\m{b}}$. For the sake of space, we omit the technical details and refer the reader to the paper we have been discussing.

\subsection{Fitting the model with available software}

\noindent There are several software packages available that will fit generalized linear mixed models using the PQL approach. The \texttt{MASS} packages contains the \texttt{glmmPQL()} function for fitting models via PQL. Packages such as \texttt{lme4} and \texttt{MCMCglmm} contain functions that fit GLMMs, but not necessarily through PQL.

In SAS, \texttt{PROC GLIMMIX} has options to fit GLMMs with the PQL method. The benefit to using PQL is that the first- and second-order approximations guarantees that parameters for most models will be estimable, a significant advantage over methods which are unable to produce estimates due to overly-complex covariance specifications.

\section{Example: epilepsy clinical trial}

Breslow and Clayton use PQL to analyze six data sets and a simulation study. We will summarize one of the examples. \cite{thall:1990} provided a data set from a clinical trial involving 59 epileptics. Patients randomly received a placebo (\texttt{Trt=0}) or a new drug (\texttt{Trt=1}). A baseline was measured as the number of epileptic seizures in the 8 week period before the study. Age in years is also included as a covariate. The logarithm of $\frac{1}{4}$ the baseline (\texttt{Base}) and the logarithm of age (\texttt{Age}) were treated as the covariates in the analysis. Every two weeks into the study, the number of seizures were counted, making the data multivariate counts. The visit times were coded as \texttt{Visit}$_i=2i-5$, for $i=1,\ldots,4$, and an indicator variable for the fourth visit (\texttt{V4=1}) is also used.

The authors have four models they compare to some of the models by Thall and Vail. The log is chosen as the link and an identity is chosen as the variance function. This results in quasi-Poisson likelihood. The general model is given by
\begin{eqnarray}
\log \mu_{jk}^b = \m{x}_{jk}^\top\m{\alpha}+b_j^1+b_j^2\mathrm{Visit}_k/10+b_{jk}^0. \label{gen}
\end{eqnarray}
The four models considered by the authors each have the fixed effects $\m{\alpha}$ and some combination of the random effects $\m{b}$. The random effects $(b_j^1, b_j^2)$ are on the subject level and follow a bivariate normal distribution. A unit level random effect $b_{jk}^0$ is used in Model III, and represents additional random error. Results are presented in Table 1.

Model I does not include any random effects, but models only the marginal effects of the mean. When compared with the other three models, we see that in those models the absolute values of the fixed effects attenuate closer to zero. The standard errors also increase with the addition of the random effects.

Models II, III, and IV include various combinations of the random effects in (\ref{gen}). Models II and III are very similar, with the exception that the coefficient and standard error for \texttt{V4} are not significant in Model III. Model IV includes a time component (\texttt{Visit}$_i$). We see that the covariance between the random effects is effectively modeled as zero. Model IV showed substantial heterogeneity among subjects. The random effects models allow us to make inference on individuals and to identify those subjects which experience marked improvements.

The authors show that the PQL method works well for this, and other, data sets by matching the results from other analyses and providing additional insights that previous approaches could not identify. More details are found in the paper.

\section{Conclusion}

We have discussed an article by \cite{breslow:1993} who describe a method for obtaining parameter effects for fixed and random effects and variance components in a generalized linear mixed model (GLMM). The penalized quasi-likelihood (PQL) uses various approximations to obtain a second-order objective function that much is easier to maximize than a full likelihood. We have filled in the gaps in the authors' theoretical development of their approach by providing details on quasi-likelihoods and Laplace's method for integral approximation. These ideas have not been covered in class and so may be difficult to understand their motivation when reading the paper.

\begin{table}
\begin{center}
\begin{tabular}{lrrrr}
 & \multicolumn{4}{c}{Model} \\
 & \mc{$I$} & \mc{$II$} & \mc{$III$} & \mc{$IV$} \\
Variable & $\mc{\hat{\beta}\pm SE}$  & $\mc{\hat{\beta}\pm SE}$ &
    $\mc{\hat{\beta}\pm SE}$ & $\mc{\hat{\beta}\pm SE}$ \\ \hline \hline
\emph{Fixed effects} & & & & \\
       Constant & $-2.76\pm.41$ & $-1.25\pm1.2$ & $-1.27\pm1.2$ & $-1.27\pm1.2$ \\
          Base  &   $.95\pm.04$ &   $.87\pm.14$ &   $.86\pm.13$ &   $.87\pm.14$ \\
            Trt & $-1.34\pm.16$ &  $-.91\pm.41$ &  $-.93\pm.40$ &  $-.91\pm.41$ \\
Base$\times$Trt &   $.56\pm.06$ &   $.33\pm.21$ &  $ .34\pm.21$ &   $.33\pm.21$ \\
            Age &   $.90\pm.12$ &   $.47\pm.36$ &   $.47\pm.35$ &   $.46\pm.36$ \\
             V4 &  $-.16\pm.05$ &  $-.16\pm.05$ &  $-.10\pm.09$ &      --- ~~~~ \\
       Visit/10 &      --- ~~~~ &      --- ~~~~ &      --- ~~~~ &  $-.26\pm.16$ \\
 & & & & \\
\emph{Random effects} & & & & \\
\emph{Subject level} & & & & \\
Const. ($\sqrt{\sigma_{11}}$) & --- ~~~~ & $.53\pm.06$ & $.48\pm.06$ & $.52\pm.06$ \\
Visit/10 ($\sqrt{\sigma_{22}}$) & --- ~~~~ & --- ~~~~ & --- ~~~~ & $.74\pm.16$ \\
Cov. ($\sigma_{12}$) & --- ~~~~ & --- ~~~~ & --- ~~~~ & $-.01\pm.03$ \\
 & & & & \\
\emph{Unit level} & & & & \\
Const. ($\sqrt{\sigma_{00}}$) & --- ~~~~ & --- ~~~~ & $.36\pm.04$ & --- ~~~~ \\ \hline
\end{tabular}
\caption{Estimates for the fixed effects $\m{\alpha}$ and covariance parameters $\m{\theta}$.}
\end{center}
\end{table}

An example using the PQL method has been summarized. The method has shown to be effective at handling complex covariance structures. \cite{fong:2009} also successfully analyze the epilepsy data using a Bayesian approach. Their work provides additional insights on GLMMs within the Bayesian framework.

In all, the PQL method works well. The authors' work in this area has been groundbreaking. Other attempts at dealing with non-normal data and covariances between random effects and between observations had not yet been met with the success as shown by Breslow and Clayton. The generality of the approach also allows many models to be fit.


\bibliography{refs}
\bibliographystyle{asa}

\end{document}
