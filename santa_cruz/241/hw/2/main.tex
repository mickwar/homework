\documentclass[12pt]{article}

\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[margin=1.00in]{geometry}
\usepackage[font=footnotesize]{caption}

\usepackage{bm}
\newcommand{\m}[1]{\mathbf{\bm{#1}}}

\usepackage[longnamesfirst]{natbib} % make it so the first citation
\bibpunct{(}{)}{;}{a}{}{,}

\begin{document}

\noindent Mickey Warner

\noindent AMS 241

\section*{Problem 1}

\subsection*{The DP mixture model}

A simulated dataset consisting of $n=250$ random draws from the mixture of normals $0.2N(-5, 1)+0.5N(0,1)+0.3N(3.5,1)$ will be analyzed in this problem. We consider the location normal Dirichlet process mixture model

\[ f(\cdot|G, \phi) = \int k_N(\cdot|\theta, \phi)dG(\theta),~~~G|\alpha,\mu,\tau^2\sim DP(\alpha, G_0=N(\mu,\tau^2)) \]

\noindent where $k_N(\cdot|\theta,\phi)$ is the density function of a normal distribution with mean $\theta$ and variance $\phi$. Hence, we are mixing over the location of the normal distribution. The hierarchical version of the model is given by 
\begin{align*}
y_i|\theta_i,\phi &\overset{ind}\sim k_N(y_i|\theta_i,\phi),~~~i=1,\ldots,n \\
\theta_i|G &\overset{iid}\sim G,~~~i=1,\ldots,n \\
G|\alpha,\mu,\tau^2 &\sim DP(\alpha, G_0=N(\mu,\tau^2)) \\
\alpha,\mu,\tau^2,\phi &\sim p(\alpha)p(\mu)p(\tau^2)p(\phi) 
\end{align*}

\noindent We let $\alpha\sim Gamma(a_\alpha, b_\alpha)$, $\mu\sim N(a_\mu, b_\mu)$, $\tau^2\sim IG(a_{\tau^2}, b_{\tau^2})$, and $\phi\sim IG(a_\phi, b_\phi)$, where the gamma has mean $a/b$ and the inverse gamma has mean $b/(a-1)$. These lead to easily sampled full conditionals.

Posterior inference is made by sampling from the marginal posterior $p(\m{\theta},\alpha,\mu,\tau^2,\phi|\m{y})$, where $\m{\theta}=(\theta_1,\ldots,\theta_n)$ are the latent mixing parameters and $\m{y}=(y_1,\ldots,y_n)$ are the data. This marginal posterior is found by integrating out the infinite-dimensional parameter $G$ from the full posterior distribution

\[p(\m{\theta},\alpha,\mu,\tau^2,\phi|\m{y}) = \int p(G,\m{\theta},\alpha,\mu,\tau^2,\phi|\m{y})dG \]

\noindent or, rather, by noting that the full posterior may be factored, using Bayes' formula, into a product of the full conditional of $G$ and the marginal posterior

\[ p(G,\m{\theta},\alpha,\mu,\tau^2,\phi|\m{y}) = p(G|\m{\theta},\alpha,\mu,\tau^2,\phi,\m{y})p(\m{\theta},\alpha,\mu,\tau^2,\phi|\m{y}). \]

\subsection*{(part 1) Full conditionals of the marginal posterior}

\noindent To simulate from $p(\m{\theta},\alpha,\mu,\tau^2,\phi|\m{y})$ we iteratively draw from the full conditionals of each parameter $\theta_1,\ldots,\theta_n,\alpha,\mu,\tau^2,\phi$. The expressions for the conditional distributions are based on the P{\'o}lya urn representation. Before we present the distributions, we introduce some notation.

Since $G$ is almost surely discrete there will be a clustering among the $\theta_i$s and the Gibbs sampler we employ takes advantage of this fact. The following list describes the notation used throughout this section:
\begin{itemize}[label=$\cdot$]
\item $n^*$ denotes the number of distinct $\theta_i$s
\item $\theta_j^*$, $j=1,\ldots,n^*$ are the distinct $\theta_i$s
\item $\m{w}=(w_1,\ldots,w_n)$ is the vector that matches each $\theta_i$ to its corresponding $\theta_j^*$, i.e., $w_i=j$ if and only if $\theta_i=\theta_j^*$
\item $n_j$ is the size of the $j$th cluster, $|\{i:w_i=j\}|$, $j=1,\ldots,n^*$
\end{itemize}

\noindent The vectors $(n^*, \m{w}, \theta_1^*,\ldots,\theta_{n^*}^*)$ and $(\theta_1,\ldots,\theta_n)$ are equivalent. The former will simplify the calculations to follow.

For each $\theta_i, i=1,\ldots,n$, the full conditional $p(\theta_i|\{\theta_k:k\neq i\},\alpha,\mu,\tau^2,\phi,\m{y})$ is given by
\[ \frac{\alpha q_0}{\alpha q_0 + \sum_{j=1}^{n^{*-}} n_j^-q_j}h(\theta_i|\mu,\tau^2,\phi,y_i)+\sum_{j=1}^{n^{*-}}\frac{n_j^-q_j}{\alpha q_0 + \sum_{j=1}^{n^{*-}}n_j^-q_j}\delta_{\theta_j^{*-}}(\theta_i) \]
\[ =Ah(\theta_i|\mu,\tau^2,\phi,y_i)+\sum_{j=1}^{n^{*-}}B_j \delta_{\theta_j^{*-}}(\theta_i) \]

\noindent where
\begin{itemize}[label=$\cdot$]
\item $q_j=k_N(y_i|\theta_j^*,\phi)$,
\item $q_0=\int k_N(y_i|\theta,\phi)g_0(\theta|\mu,\tau^2)d\theta$,
\item $h(\theta_i|\mu,\tau^2,\phi,y_i) \propto k_N(y_i|\theta_i,\phi)g_0(\theta_i|\mu,\tau^2)$,
\item $g_0$ is the density of $G_0=N(\cdot|\mu,\tau^2)$, and
\item The superscript ``$^-$'' denotes the appropriate change to $n^{*-}$, $n_j^-$, and $\theta_j^{*-}$ when omitting $\theta_i$ from their calculations.
\end{itemize}

\noindent We update $p(\theta_i|\{\theta_k:k\neq i\},\alpha,\mu,\tau^2,\phi,\m{y})$, for $i=1,\ldots,n$, sequentially by drawing either (1) a new value from $h$ with probability $A$, or (2) $\theta_j^*$ with probability $B_j$ ($A+B_1+\cdots+B_{n^{*-}}=1$). With each update of $\theta_i$ we also update the clustering ``parameters'' $n^*$, $\theta_j^*$, and $n_j$, for $j=1,\ldots,n^*$ ($\m{w}$ is more or less for bookkeeping and isn't explicitly used in the sampling algorithm).

Note that to use the Gibbs sampler we require conjugacy with $k_N$ and $G_0$. Without conjugacy we would have to resort to other methods for updating $\theta_i$, say an algorithm from \cite{neal2000markov}.

The functional form of $q_j$ is simply a normal density
\begin{align*}
q_j = (2\pi\phi)^{-1/2}\exp\left\{-\frac{1}{2\phi}(y_i - \theta_j^*)^2\right\}
\end{align*}

We solve for $q_0$ by integrating out $\theta$ (which has a normal kernel) and re-arranging terms to simplify to a nice normal density
\begin{align*}
q_0 &= \int (2\pi\phi)^{-1/2}\exp\left\{-\frac{1}{2\phi}(y_i-\theta)^2\right\}(2\pi\tau^2)^{-1/2}\exp\left\{-\frac{1}{2\tau^2}(\theta-\mu)^2\right\}d\theta \\
 &= (4\pi^2\phi\tau^2)^{-1/2} \int \exp\left\{-\frac{1}{2\phi}(y_i-\theta)^2-\frac{1}{2\tau^2}(\theta-\mu)^2\right\}d\theta \\
 &= (4\pi^2\phi\tau^2)^{-1/2} \int \exp\left\{-\frac{1}{2\phi\tau^2}\left[\tau^2y_i^2-2y_i\tau^2\theta +\tau^2\theta^2 + \phi\mu^2 - 2\mu\phi\theta + \phi\theta^2\right]\right\}d\theta \\
 &= (4\pi^2\phi\tau^2)^{-1/2} \int \exp\left\{-\frac{1}{2\phi\tau^2}\left[ \theta^2(\phi+\tau^2) -2\theta(\mu\phi+y_i\tau^2)\right] -\frac{\phi\mu^2+\tau^2y_i^2}{2\phi\tau^2}\right\}d\theta \\
 &= (4\pi^2\phi\tau^2)^{-1/2}\exp\left\{-\frac{\phi\mu^2+\tau^2y_i^2}{2\phi\tau^2}\right\} \int \exp\left\{-\frac{\phi+\tau^2}{2\phi\tau^2}\left[ \theta^2 -2\theta\frac{\mu\phi+y_i\tau^2}{\phi+\tau^2}\right]\right\}d\theta \\
 &= (4\pi^2\phi\tau^2)^{-1/2}\exp\left\{-\frac{\phi\mu^2+\tau^2y_i^2}{2\phi\tau^2}\right\} \int \exp\left\{-\frac{1}{2\sigma^*}\left[ \theta^2 -2\theta \mu^* +\mu^{*2} - \mu^{*2}\right]\right\}d\theta \\
& \mathrm{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(where~\mu^*=\frac{\mu\phi+y_i\tau^2}{\phi+\tau^2},~and~\sigma^* = \frac{\phi\tau^2}{\phi + \tau^2})} \\
 &= (4\pi^2\phi\tau^2)^{-1/2}\exp\left\{-\frac{\phi\mu^2+\tau^2y_i^2}{2\phi\tau^2}\right\} (2\pi\sigma^*)^{1/2}\exp\left\{\frac{\mu^{*2}}{2\sigma^*}\right\} \\
 &= (2\pi\phi\tau^2)^{-1/2}\left(\frac{\phi\tau^2}{\phi+\tau^2}\right)^{1/2}\exp\left\{-\frac{\phi\mu^2+\tau^2y_i^2}{2\phi\tau^2} + \frac{(\mu\phi+y_i\tau^2)^2}{2\phi\tau^2(\phi+\tau)}\right\} \\
 &= (2\pi(\phi+\tau^2))^{-1/2}\exp\left\{\frac{-(\phi\mu^2+\tau^2y_i^2)(\phi+\tau^2) + (\mu\phi+y_i\tau^2)^2}{2\phi\tau^2(\phi+\tau^2)}\right\} \\
 &= (2\pi(\phi+\tau^2))^{-1/2}\exp\left\{\frac{-\phi^2\mu^2-\phi\mu^2\tau^2-\phi y_i^2\tau^2-y_i^2\tau^2 + \mu^2\phi^2+2\mu\phi y_i\tau^2 + y_i^2\tau^2}{2\phi\tau^2(\phi+\tau^2)}\right\} \\
 &= (2\pi(\phi+\tau^2))^{-1/2}\exp\left\{\frac{-\phi\mu^2\tau^2-\phi y_i^2\tau^2+2\mu\phi y_i\tau^2}{2\phi\tau^2(\phi+\tau^2)}\right\} \\
 &= (2\pi(\phi+\tau^2))^{-1/2}\exp\left\{-\frac{y_i^2 - 2y_i\mu + \mu^2}{2(\phi+\tau^2)}\right\} \\
 &= N(y_i|\mu, \phi+\tau^2)
\end{align*}

\noindent And thus the bookkeeping was worth it. More importantly, note the conjugacy requirement for the integral to be tractable.

The density function $h(\theta_i|\cdot)$ has essentially already been derived when finding $q_0$. After dropping all the non $\theta_i$ terms, we are left with the part from $q_0$ that was inside the integral. That is, $h$ is a normal distribution with mean $\mu^*$ and variance $\sigma^*$ given above. This completes the marginal posterior for $\theta_i$.

The conditionals for the other parameters are much easier in comparison. We update $\alpha$ by first drawing an auxiliary variable $\eta|\alpha\sim Beta(\alpha+1,n)$ and then drawing from the conditional 
\[\alpha|\eta,n^*,\m{y}\sim\epsilon Gamma(a_\alpha+n^*, b_\alpha-\log\eta) + (1-\epsilon)Gamma(a_\alpha+n^*-1,b_\alpha-\log\eta)\]
\noindent where $\epsilon=(a_\alpha+n^*-1)/(n(b_\alpha-\log\eta)+a_\alpha+n^*-1)$. Draws for $\eta$ are discarded. This update works since $\alpha$ has a gamma prior.

The joint conditional $p(\mu, \tau^2|\m{\theta}^*, n^*, \m{y})\propto p(\mu,\tau^2)\prod_{j=1}^{n^*}g_0(\theta_j^*|\mu, \tau^2)$ is basically the canonical example of Gibbs sampling. Because of our choice of priors and $G_0$, we will update $\mu$ and $\tau^2$ separately. For $\mu|\cdot$, we have
\begin{align*}
p(\mu|\tau^2,\m{\theta}^*, n^*, \m{y}) &\propto \exp\left\{-\frac{1}{2b_\mu}(\mu-a_\mu)^2\right\}\exp\left\{-\frac{1}{2\tau^2}\sum_{j=1}^{n^*}(\theta_j^*-\mu)^2\right\} \\
&\propto \exp\left\{-\frac{1}{2b_\mu}(\mu^2-2a_\mu\mu+a_\mu^2)-\frac{1}{2\tau^2}(\sum_{j=1}^{n^*}\theta_j^{*2}-2\mu\sum_{j=1}^{n^*}\theta_j^*+n^*\mu^2)\right\} \\
&\propto \exp\left\{-\frac{1}{2b_\mu}(\mu^2-2a_\mu\mu)-\frac{1}{2\tau^2}(-2\mu S+n^*\mu^2)\right\},~~~~~(S=\sum_{j=1}^{n^*}\theta_j^*) \\
&\propto \exp\left\{-\frac{1}{2b_\mu\tau^2}(\mu^2\tau^2-2a_\mu\mu\tau^2 - 2\mu Sb_\mu + \mu^2n^*b_\mu)\right\} \\
&\propto \exp\left\{-\frac{1}{2b_\mu\tau^2}(\mu^2(\tau^2+b_\mu n^*) -2\mu(a_\mu\tau^2 + b_\mu S))\right\} \\
&\propto \exp\left\{-\frac{\tau^2+b_\mu n^*}{2b_\mu\tau^2}\left[\mu^2 -2\mu\left(\frac{a_\mu\tau^2 + b_\mu S}{\tau^2+b_\mu n^*}\right)\right]\right\} \\
&\propto \exp\left\{-\frac{1}{2\sigma'}(\mu-\mu')^2\right\},~~~~~(\mu'=\frac{a_\mu\tau^2+b_\mu S}{\tau^2+b_\mu n^*},~\sigma'=\frac{b_\mu\tau^2}{\tau^2+b_\mu n^*})
\end{align*}
Thus $\mu|\cdot \sim N(\mu', \sigma')$. For $\tau^2|\cdot$, we have
\begin{align*}
p(\tau^2|\mu,\m{\theta}^*, n^*, \m{y}) &\propto (\tau^2)^{-a_{\tau^2}-1}\exp\left(-\frac{b_{\tau^2}}{\tau^2}\right)(\tau^2)^{-n^*/2}\exp\left\{-\frac{1}{2\tau^2}\sum_{j=1}^{n^*}(\theta_j^*-\mu)^2\right\} \\
&\propto (\tau^2)^{-(a_{\tau^2}+n^*/2)-1}\exp\left\{-\frac{1}{\tau^2}\left(b_{\tau^2}+\frac{1}{2}\sum_{j=1}^{n^*}(\theta_j^*-\mu)^2\right)\right\}
\end{align*}
which is an $IG(a_{\tau^2}+n^*/2, b_{\tau^2}+\frac{1}{2}\sum_{j=1}^{n^*}(\theta_j^*-\mu)^2)$

Finally, we need the conditional distribution for $\phi$. We have
\begin{align*}
p(\phi|\cdot) &\propto p(\phi)\prod_{i=1}^nk(y_i|\theta_i,\phi) \\
&\propto \phi^{-a_\phi-1}\exp\left(-\frac{b_\phi}{\phi}\right)\prod_{i=1}^n \phi^{-1/2}\exp\left\{-\frac{1}{2\phi}(y_i-\theta_i)^2\right\} \\
&\propto \phi^{-(a_\phi+n/2)-1}\exp\left\{-\frac{1}{\phi}\left(b_\phi+\frac{1}{2}\sum_{i=1}^n(y_i-\theta_i)^2\right)\right\}
\end{align*}
which is an $IG(a_\phi+n/2, b_\phi+\frac{1}{2}\sum_{i=1}^n(y_i-\theta_i)^2)$. This completes the formulation for all necessary full conditionals to sample from the marginal posterior.

An additional, but not necessary, step is included to improve the sampling of the cluster locations $\theta_j^*$. It can be shown that the full conditional for each $\theta_j^*$ is normally distributed with mean and variance
\[ \frac{\phi\mu+\tau^2\sum_{\{i:w_i=j\}}y_i}{\phi+n_j\tau^2},~~~~~\frac{\phi\tau^2}{\phi+n_j\tau^2} \]
Including this step basically rocked my world when I saw how much better the clustering was.

\subsection*{(part 2) Priors for $\phi$, $\mu$, and $\tau^2$}

The priors I decided upon for the analysis are:
\begin{itemize}[label=$\cdot$]
\item $\phi$ -- $a_\phi=3$, $b_\phi=10$ (mean 5, variance 25)
\item $\mu$ -- $a_\mu=0$, $b_\mu=3$ (mean 0, variance 3)
\item $\tau^2$ -- $a_{\tau^2}=3$, $b_{\tau^2}=10$ (mean 5, variance 25)
\end{itemize}
Plots of the posteriors (Fig. 1), along with predictive distributions (Fig. 5), are given at the end of this section (just before problem 2). In choosing these hyperparameters, I wanted something a bit non-informative, but still small since I knew the data didn't contain anything very extreme. The posterior predictive distributions for a new cluster $\theta_0$ and data point $y_0$ seem to appropriately model the data. I did consider other choices of hyperparameters, and these are discussed now.

I do not show the plots for the following analyses. I ran the sampler with some extreme values for the hyperparameters:
\begin{itemize}[label=$\cdot$]
\item $\phi$ -- $a_\phi=3$, $b_\phi=90$ (mean 45, variance 2025)
\item $\mu$ -- $a_\mu=10$, $b_\mu=0.1$ (mean 10, variance 0.1)
\item $\tau^2$ -- $a_{\tau^2}=2.1$, $b_{\tau^2}=0.1$ (mean 0.09, variance 0.08)
\end{itemize}
Interestingly, the posterior for $\tau^2$ was much higher than its prior, having a 95\% hpd of (4.6, 130.7); $\phi$ was bimodal around 5 and 10; and $\mu$ was concentrated close to 10. The number of posterior clusters also dropped dramatically (concentrated around 1 cluster). The posterior predictive showed the new $\theta_0$ be at the three ``main'' locations (-5, 0, 3.5) of the original prior selection, however the predictive $y_0$ was now unimodal around 0 with variance 10.

Again, I ran the sampler with the previous list of hyperparameters, except I changed $b_\mu$ to equal 10. This time, the bimodality in $\phi$ went away, however it was now centered around 10. The posteriors for $\alpha$, $\mu$, $n^*$, and $\tau^2$ went to much more reasonable values (close to what we see in the plots). The posterior predictive distribution for $\theta_0$ was unimodal and $y_0$ was about the same as before. $\phi$ was still too high, which is why I was seeing poor predictions. Since $\phi$ was high (10, when it should really be closer to 1), there was no emphasis on creating clusters away from $\mu$.

I believe my choice of priors (the original list) to be reasonable as they yield appropriate posterior predictive distributions.

\subsection*{(part 3) Prior for $\alpha$}

In choosing several different hyperparameters for $\alpha$, I noticed that the prior had a large impact on the number of distinct clusters $n^*$. For the analysis, I a selected $a_\alpha=b_\alpha=1$, which gives a prior mean of $\alpha$ to be $1$ and variance $1$. This in turn gives an approximate prior mean for $n^*$ to be about $6.1$.

The posterior distribution for $\alpha$ is shown in Fig. 2.

Having a higher prior mean and variance for $\alpha$ did increase the posterior mean and variance. However, the number of clusters $n^*$ and the posterior predictive distributions were not affected by this change. When I put a very tight, low prior for $\alpha$, one with very small mean and variance, the posterior mean for $n^*$ was much smaller, but the posterior predictive distributions remained about the same. This leads me to believe that though $\alpha$ plays a key role in the posterior of $n^*$, it does not have so much of an impact on the other parameters in the model nor on the predictions.

\subsection*{(part 4) Posteriors for $n^*$ and $\theta_i$}

Fig. 2 shows the posterior for $n^*$ (top left) and a heatmap of how the cluster labels $\theta_i$ were grouped (top right). In Fig. 3, I show the mean and equal-tailed 95\% credible interval for each $\theta_i$. Together, these figures show that there are three main clusters that the $\theta_i$'s fall in.

The three main groups might seem contradictory to what we're seeing in the posterior for $n^*$. What is happening is that when new clusters are formed, they are still around the main groups.

In Fig. 5 (left) we see the posterior predictive distribution of a new cluster label $\theta_0$. A simple look at a kernel density estimate of the data would suggest that the distributional shape of $\theta_0$ is appropriate.

\subsection*{(part 5) Posterior predictive for a new $\theta_0$ and new $y_0$}

In Fig. 4 I show the prior predictive distributions for a new cluster $\theta_0$ and a new observation $y_0$. My prior specification results in fairly widespread normal predictive distributions. On the right, I overlay a KDE of the data. This shows that my priors clearly do not capture the shape of the data.

In Fig. 5 we see the posterior predictive distributions for a new cluster and a new observation. Now, we are able to nicely capture the cluster locations as well as the shape of the data. In these plots and in previous ones with a shaded density, the darker blue region denotes the 95\% highest posterior density set. In the predictive distribution of $\theta_0$ we have a set made up of three intervals.

For model comparison purposes, I use the posterior predictive loss criterion: $D(M)=P(M)+\frac{k}{k+1}G(M)$ where $k>0$, $P(M) = \sum_{i=1}^n \Var(y_{0,i}|data)$, and $G(M) = \sum_{i=1}^n[y_i-E(y_{0,i}|data)]^2$. I prefer to have an equal weight between the variance $P$ and goodness-of-fit $G$ components, so $k=\infty$. This model yielded $D(M)=4707.39$.
 
\newpage

%\subsection*{Plots for problem 1}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.42]{figs/posts_1.pdf}
\caption{Trace plots and posterior distributions for $\phi$, $\mu$, and $\tau^2$}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.45]{figs/nstar_1.pdf}
\caption{Top left: posterior distribution for the number of distinct labels, or clusters, $n^*$. Top right: a heatmap showing how often two labels (or observations) were part of the same cluster; blue denotes fewer times in common and red is for very often paired together. Bottom: (left) trace plot and (right) posterior distribution (and 95\% hpd) for $\alpha$.}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.75]{figs/lines_1.pdf}
\caption{This figure shows each cluster label $\theta_i$, sorted by the order of the observations. Black: equal-tailed 95\% credible intervals for each $\theta_i$. Green: posterior mean for each $\theta_i$. Red: the observations.}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{figs/prior_1.pdf}
\caption{\emph{Prior} predictive distributions for a new cluster $\theta_0$ and new data point $y_0$}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{figs/pred_1.pdf}
\caption{\emph{Posterior} predictive distributions for a new cluster $\theta_0$ and new data point $y_0$. Note the appearance of three ``main'' clusters. Though the posterior for the \emph{number} of clusters $n^*$ showed more than three clusters, most of these were still around the three modes.}
\end{center}
\end{figure}


\newpage

\section*{Problem 2}

\subsection*{Location-scale normal DP mixture model}

We fit the same $n=250$ observations as in problem 1 to the following model

\[ f(\cdot|G) = \int k_N(\cdot|\theta,\phi)dG(\theta,\phi), ~~~~~~ G|\alpha,\m{\psi} \sim DP(\alpha, G_0(\m{\psi}), \]
where the baseline distribution is given by
\[ G_0(\theta,\phi|\m{\psi}) = Normal(\theta|\mu,\phi/\kappa)InvGamma(\phi|c, \beta) \]
with $c=3$ fixed. I tried to specify comparable, noninformative priors for this model as the one in problem 1. I let $\mu\sim N(0, \sqrt{3}^2)$, $\kappa\sim Gamma(1,1)$, $\beta\sim InvGamma(5, 10)$}. Perhaps the least intuitive part of this model is the role that $\phi$ plays as it now appears in both parts of $G_0$. From the previous analysis, I expect $\phi$ to be around $1$ despite the added complexity. For $\phi$, the prior mean is about $1$ and prior variance is about $3$.

This specification does put quite a bit of mass on the tails for the (implicit) prior of $\theta$, having a variance in the tens of thousands, but there didn't seem to be any issues in the posterior distribution that would make me seriously question my choice of priors. In fact, the posterior predictions were very similar to that from model 1 (Fig. 6). For this model $D(M)=4745.31$, which indicates a very slight preference toward model 1, but this is nearly negligible.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{figs/pred_2.pdf}
\caption{Left: contour plot of the joint posterior distribution of the mixing parameters $\theta,\phi$. Right: posterior predictive distribution for a new data point $y_0$ of the location-scale mixture model.}
\end{center}
\end{figure}

\newpage

\section*{Problem 3}

\subsection*{Poisson regression}

We consider a new data set consisting for $n=32$ observations of $(x_i, y_i)$ pairs. The data are incidents of faults in the manufacturing of rolls of fabric. The covariate $x_i$ is the length of the roll (which I scale down by 100), and the response $y_i$ is the number of faults.

We will compare three models in this section. Posterior distributions are sampled using a tuned Metropolis algorithm for models 1 and 2. Model 3, which has a DP mixture, uses the Gibbs sampler as in problem 1, and a Metropolis step on the parameters without conjugate priors. There didn't seem to be any issue with convergence in our sampler.

\subsection*{Model 1, simple Bayesian model}

The first model is written
\[ y_i|x_i,\theta,\beta \sim Poisson(\theta\exp(\beta x_i)), ~~~~~ i=1,\ldots,32 \]
I choose $\theta \sim Gamma(4, 1)$ and $\beta \sim Normal(0, 1)$. I believe these priors to put mass on plausible values for the parameters. Since the $\beta$ is exponentiated, this is a fairly noninformative prior for $\beta$.

The table below gives a summary of the posterior distributions for $\theta$ and $\beta$.

\begin{center}
\begin{tabular}{lrrrrrr}
         &      & \multicolumn{5}{c}{Quantiles} \\
         & Mean & \multicolumn{1}{|r}{0\%}  & 2.5\% & 50\% & 97.5\% & 100\% \\ \hline\hline
$\theta$ & 2.81 &  1.23 & 1.82  & 2.77 &  3.97  & 5.35  \\
$\beta$  & 0.18 &  0.08 & 0.13  & 0.18 &  0.24  & 0.29  \\ \hline
\end{tabular}
\end{center}

Figure 7 shows the posterior predictions in two ways: first, as a plot of the predictions at each covariate from the data set; and second, as a plot of the predictions against their observed values. For the second plot, an ideal model would have the mean predictions (green dots) be very close to the dotted line and small errors (green bars). For this model $D(M)=955.09$, with $P(M)\approx 300$ and $G(M)\approx 650$.

\subsection*{Model 2, hierarchical Bayesian model}

We extend the first model to a hierarchical formulation,
\begin{align*}
y_i|x_i,\theta_i,\beta &\sim Poisson(\theta_i\exp(\beta x_i)), ~~~~~ i=1,\ldots,32 \\
\theta_i|\zeta, \mu &\sim Gamma(\zeta, \zeta\mu^{-1}) 
\end{align*}
This model allows us to account for overdispersion, relative to the Poisson, in the data. I choose $\zeta \sim Gamma(1, 1/2)$ and $\mu\sim Gamma(1, 1/2)$. This yields a prior mean for $\theta_i$ to be about $2$ and a variance of around $30$. I use the same prior for $\beta$ as in model 1.

\newpage

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{figs/pred_3a.pdf}
\caption{Model 1. Left: a plot of the model predictions for each $x_i$ in the data set. Right: a plot of the observed versus fitted values. The green bars indicate 95\% predictive regions, and the green circles are the predictive means. In the right plot, a good model should have the bars crossing the dotted line (which indicates observed equals fitted).}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{figs/pred_3b.pdf}
\caption{Model 2. Posterior predictions, as in model 1's figure. However, to make predictions for a new observation $y_0$ we must now generate a new $\theta_0$ based on the posteriors for $\zeta$ and $\mu$.}
\end{center}
\end{figure}

\newpage

Since there are now $n=32$ $\theta_i$'s, I will not show a summary of them. But a summary of $\beta$, $\zeta$, and $\mu$ are given in the following table

\begin{center}
\begin{tabular}{lrrrrrr}
         &      & \multicolumn{5}{c}{Quantiles} \\
         & Mean & \multicolumn{1}{|r}{0\%}  & 2.5\% & 50\% & 97.5\% & 100\% \\ \hline\hline
 $\beta$ & 0.19 & 0.05 & 0.10 & 0.19 &  0.27 &  0.31 \\
 $\zeta$ & 6.11 & 1.54 & 2.90 & 5.72 & 11.53 & 15.97 \\
 $\mu$   & 2.74 & 1.05 & 1.54 & 2.60 &  4.50 &  6.30 \\ \hline
\end{tabular}
\end{center}
Predictions for this model are shown in Figure 8. Note that we do not use the posteriors of each $\theta_i$ to make the predictions at each $x_i$. Doing this would imply we know the random effect for a particular observation, but we wish to make a prediction for a new observable whose random effect is unknown. Hence, we first generate $\theta_0$ based on the posteriors of $\zeta$ and $\mu$ and then generate a new $y_0$ based on $\theta_0$.

It is readily seen that the variance in the predictions has increased and now cross the dotted line. This increase in variance leads to $D(M)=1538.68$ with $P(M) \approx 900$ and $G(M) \approx 650$. So the goodness-of-fit term is about the same as in model 1, but we are penalized more for the variance. This may seem to favor model 1 over model 2, but recall that prediction bands for model 1 struggled to cross the dotted line. This further suggests we should be using other criteria for model comparison. Another time, another day, that's what I always say.

\subsection*{Model 3, semiparametric DP mixture}

The model is extended again to a semiparametric DP mixture, similar to that from problem 1. We write
\begin{align*}
y_i|x_i,\theta_i,\phi &\overset{ind}\sim Poisson(y_i|\theta_i\exp(\beta x_i)),~~~i=1,\ldots,n \\
\theta_i|G &\overset{iid}\sim G,~~~i=1,\ldots,n \\
G|\alpha,\zeta,\mu &\sim DP(\alpha, G_0=Gamma(\zeta,\zeta\mu^{-1})) \\
\alpha,\mu,\tau^2,\phi &\sim p(\alpha)p(\mu)p(\tau^2)p(\phi) 
\end{align*}
This model includes as limiting cases both models 1 and 2. I use the same priors for $\beta$, $\zeta$, and $\mu$ as in model 2. I let $\alpha\sim Gamma(1, 1/2)$. The same kind of tedious details described in problem 1 apply here as well (e.g. solving for the full conditional of each $\theta_i$), but I omit the details for brevity. A summary for $\alpha$, $\beta$, $\zeta$, and $\mu$ are given in the following table:
\begin{center}
\begin{tabular}{lrrrrrr}
         &      & \multicolumn{5}{c}{Quantiles} \\
         & Mean & \multicolumn{1}{|r}{0\%}  & 2.5\% & 50\% & 97.5\% & 100\% \\ \hline\hline
$\alpha$ & 2.64 & 0.00 & 0.23 & 2.02 & 8.50 & 24.50 \\ 
$\beta$ & 0.19 & 0.01 & 0.10 & 0.18 & 0.27 &  0.36 \\
$\zeta$ & 3.88 & 0.03 & 0.75 & 3.46 & 9.27 & 18.19 \\
$\mu$   & 2.96 & 0.29 & 1.26 & 2.75 & 5.85 & 13.50 \\ \hline
\end{tabular}
\end{center}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{figs/pred_3c.pdf}
\caption{Model 3. Similar to the figures for models 1 and 2.}
\end{center}
\end{figure}

Plots of the predictions for model 3 are shown in Figure 9, comparable to Figures 7 and 8 for the other two models. The predictions look surprisingly close to the hierarchical model's. Here, $D(M)=1421.57$ with $P(M)\approx 770$ and $\G(M)\approx 650$. For all three models, the goodness-of-fit term $G(M)$ is practically equivalent. The only difference being the penalty or variance term $P(M)$. Model 1 had the smallest variance but 95\% predictive intervals didn't correctly capture the observations. Whereas in models 2 and 3 we see an increase in variance to accommodate the poor predictions. My preference, in this case, would be toward the DP mixture model.


\bibliography{refs}
\bibliographystyle{asa}


\end{document}
